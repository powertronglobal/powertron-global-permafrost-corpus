{"chunk_id": "umontana-milestone2-validation_chunk_0001", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 1, "text": "M598 ‚Äì 498 Milestone 2 Report \nStudents: [Student Researcher D], [Student Researcher E], [Student Researcher F] \nFaculty Advisors: [Faculty Advisor A], [Faculty Advisor B] \n \nIncorporating Machine Learning with Cloud Based HVAC Monitoring \nSystems \n \nIntroduction \n \nAs a final step in the M598-498 University of Montana Capstone, the team tested several machine \nlearning and data science algorithms with the data given at the beginning of the semester. The goal \nof this exploration was to practice real world implementation of learned data science models while \ndetermining if initial results from any of the selected methods produced successful detection of \nanomalies within the dataset. Anomaly detection within the machine sensor data given to use posed \nobvious use cases when the exploration began, from real-time detection of machine inefficiencies \nand excessive energy use to the possibility of predictive mechanical failure. \n \nIn the following sections you will find: \n- \nAn outline of feature extraction and treatment of the data \n- \nSummaries of a variety of machine learning and data science techniques, including:  \no \nRandom Forest Modeling \no \nMedian Absolute Deviation from the Median (MAD) \no \nAgglomerative Hierarchical Clustering \no \nArchetypes Analysis \n- \nAn exploration of the process of building simulated data \n- \nOverall Limitations, Variables to Measure, and Conclusion \n \nConclusions are discussed in detail in the limitations and potential use cases and suggestions, \nhowever success came largely from examining data sets pre and post-treatment by PowerTron. The \nlack of labelled data rendered the training of supervised models and assessment of results difficult. \nThe team was able to explore whether algorithms could identify pre and post-treatment data in an \neffective fashion as the data was labeled as pre and post treatment. Possible success in \nidentification of anomalies in the machine‚Äôs behavior is seen with threshold analyses of certain \nfeatures, however this model‚Äôs lack of mechanism for confirming results produces immature findings \nthat are difficult to verify. \n \nSection I. Feature Extraction and Exploration \n \nFeature Extraction   \n  \nIn determining whether an HVAC system is performing within spec using sensor data, the team had \nto first determine which features should be used to best characterize the device‚Äôs performance.  \nFeature extraction represents the process of choosing which criteria will be used to make this \ndecision, aiding in any pre-treatment applied to the data.  \n \nInitial meetings and reports provided by PowerTron combined with general research on HVAC \nfunction narrowed the teams approach to a focus on kW/ton, COP, and EER as appropriate \nmeasures of device efficiency.", "tokens": 529, "chars": 2721, "pages": {"start": 1, "end": 1}, "section": "", "prev_id": "", "next_id": "umontana-milestone2-validation_chunk_0002", "content_hash": "sha256:bfec56239050f06c8ef5c6a5098570b4bb274f0fa306c60cd23102f43cdf0721"}
{"chunk_id": "umontana-milestone2-validation_chunk_0002", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 2, "text": "decision, aiding in any pre-treatment applied to the data.  \n \nInitial meetings and reports provided by PowerTron combined with general research on HVAC \nfunction narrowed the teams approach to a focus on kW/ton, COP, and EER as appropriate \nmeasures of device efficiency. \n\n \n \nPage 2 of 38 \n \nCapacity was also examined as it has an implied value related compressor function. Capacity can be \nconsidered as a measure of the ability of the device to provide a certain amount of cooling. Based on \nmanufacturing standards, there are expected values for capacity when one compressor is \nfunctioning, two compressors are functioning, and so on. Also related to compressor function was \ndeveloping an understanding of extracting the amount of time one compressor, two compressors, \nthree compressors, and four compressors were running in a given time period. This can be \ndesignated by either the total number of seconds a device was running during a day or by producing \nthe average cycle runtime, manifested as a percentage. \n \nOutdoor air temperature has an expected relationship with certain key variables mentioned above, \nsuch as the kW/ton used by the machine. When the temperature outside increases, the amount of \nenergy required to provide cooling capabilities increases.  \n \nIncluded in the determination of which features to extract is the exploration of statistical \nrepresentations of each measure over a given time period. Certain measurements are less impacted \nby sensor errors or expected spikes (outliers) in calculated measurements due to the common \nnature of equipment function.  \n \nFor instance, kW/ton is expected to spike when a device cycles and compressors turn on or off. \nWhen a device turns on, it‚Äôs capacity is close to zero, so the calculation results in the kW being \ndivided by a very small number, which results in a very large kW/ton measurement. An effective \nstatistical approach to handling spikes in calculated measurements is to take the median, as \nopposed to the mean (average), as it is less impacted by outliers in very large data sets. Hence, \nseveral features explored include the median kW/ton, COP, and the EER. \n \nThe variability of the data provided was also assessed during feature extraction.  Typical statistics \nfor the assessment of variability consist of the range, the interquartile range (the lower and upper \nboundaries of the middle 50% of the data), and the standard deviation.  Unfortunately, the mean and \nstandard deviation (based on the mean) are impacted by ‚Äòcommon‚Äô outliers in this data set and were \ndiscarded.  The range, which reflects the minimum and maximum values in a given data set, was \nused.  The interquartile range was also used to calculate a useful feature known as \"percentage of \noutliers in a day\" (calculated by dividing the number of datum categorized as outliers by the number \nof points in the entire set under examination). \n \nOutliers indicate unusual values, and a point is classified as an outlier if it is further than some \ndefined distance away from the median, known as thresholding. This is a categorical feature, which \nallows for determining whether a datum is or is not an outlier.  Related to outliers and the creation of \nthresholds, the median absolute deviation from the median (MAD) was calculated and used as a \nfeature.  Much like standard deviation, which portrays the average distance from the mean, MAD \nillustrates the typical distance from the median of the data. \n \nFinally, by exploring the relationship between data points, one can produce additional features \nbased on the identified relationships.  Particularly fruitful was the examination of the relationship \nbetween outside air temperature and average cycle runtime. The expected relationship would model \nan increase in cycle runtime with increases in temperature i.e. the device has to run for longer if it is", "tokens": 789, "chars": 3881, "pages": {"start": 1, "end": 2}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0001", "next_id": "umontana-milestone2-validation_chunk_0003", "content_hash": "sha256:0824bc61821b6f14c884242901868e4467b6ee7e6972204c84bd1950f650964d"}
{"chunk_id": "umontana-milestone2-validation_chunk_0003", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 3, "text": "xploring the relationship between data points, one can produce additional features \nbased on the identified relationships.  Particularly fruitful was the examination of the relationship \nbetween outside air temperature and average cycle runtime. The expected relationship would model \nan increase in cycle runtime with increases in temperature i.e. the device has to run for longer if it is \n\n \n \nPage 3 of 38 \n \nhotter outside. Preliminary results showed that runtime seemed to increase linearly with outside air \ntemperature. \nBy demonstrating a linear relationship between these two variables, linear regression allows for the \ncreation an equation to model this relationship.  This equation was used to create a feature based on \nthe deviation from the predicted relationship, where the deviation is referred to as an error.  If the \nerror was consistently large, this would merit attention, as it could mean that the machine was not \nperforming as expected. \n \nEach of the sub-sections under Section II: Deep Dive Methods, references the specific pre-treatment \nof data and the features extracted for each model tested. Feature extraction represents a key piece \nin any data science or machine learning implementation, as its impacts can range from the actual \ndetermination of which technique to explore to the actual success of the technique itself.  \n \nThe team believes that further features could have been explored in future exploration specifically \nrelated to the features and variables that one would need to assess for successful predictive \nmechanical failure. \n \nSection II. Deep Dive Methods \n \nThe following subsections outline the methods explored further by the team given time constraints. \nEach section produced different results and conclusions surrounding the effectiveness of any of the \ngiven algorithms or methods are independent of one another. Common limitations and conclusions \nresulting from all deep dives are discussed in the final section of this paper. \n \nThe Random Forest Algorithm \n \nMethod Description \n \nThe random rorest is a classification algorithm that will take a large set of inputs and put them into \ndifferent categories. It is a \"supervised\" algorithm, which means that there must be a training set of \ndata which allows for the machine to categorize future data sets based on given sample inputs and \noutputs. This is called labeled data.  Consider a data set gathered by surveying people on whether \nthey would buy a certain cell phone. \n \n \n \nPhone\nPrice\nRAM\nInternal Storage\nLabel\nA \n$700 \n32 GB \n128 GB \nDon't Buy \nB \n$350 \n16 GB \n128 GB \nBuy \nC \n$90 \n1 GB \n4 GB \nDon't buy \n‚Ä¶ \n‚Ä¶ \n‚Ä¶ \n‚Ä¶ \n‚Ä¶ \n \nEach input is a row representing a phone and can be thought of as a single data point with three \nfeatures: price, RAM and internal storage.  The key desired output is whether someone would buy", "tokens": 571, "chars": 2841, "pages": {"start": 2, "end": 3}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0002", "next_id": "umontana-milestone2-validation_chunk_0004", "content_hash": "sha256:945e70964ff4539c0738fc0a36b23d4689f88352d5d1f114173381ab6ced6cba"}
{"chunk_id": "umontana-milestone2-validation_chunk_0004", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 4, "text": "GB \nDon't Buy \nB \n$350 \n16 GB \n128 GB \nBuy \nC \n$90 \n1 GB \n4 GB \nDon't buy \n‚Ä¶ \n‚Ä¶ \n‚Ä¶ \n‚Ä¶ \n‚Ä¶ \n \nEach input is a row representing a phone and can be thought of as a single data point with three \nfeatures: price, RAM and internal storage.  The key desired output is whether someone would buy \n\n \n \nPage 4 of 38 \n \nthis phone, so each row is labeled \"buy\" or \"don't buy.\" \n \nThe basis for a machine running a random forest algorithm is the use of decision trees. Figure 1 \nrepresents an example of a simple decision tree based on the above data.1 \n \nFigure 1 - Simple Decision Tree \n \n \n \nDecision trees are fast, easy to understand, and work well with both non-numeric data (e.g., color, \nbrand) and numerical data (price, internal storage etc.).  However, decision trees carry certain \nlimitations. They can be inflexible; for example, in the tree above, there is no option to allow for a \nphone more than $500 to be purchased.  This inflexibility produces a bias in the characterization of \nthe data, though it may have simply occurred because no respondents represented in the provided \ntraining set wanted to buy an expensive phone. Additionally, decision trees are very sensitive to \nsmall differences in the training data provided \n \nThe random forest algorithm improves on decision trees, because it is an ensemble method. This \nmeans it uses many methods or trials to make many (ideally) independent predictions and combines \nthem to form one prediction. The intuition here is that having many independent predictions reduces \nthe bias that can come from the training set (the one above is biased against expensive phones) and \ncopes better with the natural variability found in the real world. \n \nThe random forest algorithm uses smaller decision trees and then determines which outcome was \nmost common for its output.   \n \nIn the example above, the smaller decision trees could be formed by random subsets of the original \ntree. For example, one tree might be just a one node tree (Figure 2), that is one level deep; another \nmight be a two node, two level deep tree (Figure 3); the final subset represents a third tree (Figure 4) \nthat was included in Figure 3, as duplication of nodes between the trees poses no issue. \n \n1 https://www.simplilearn.com/ice9/free_resources_article_thumb/phone-price.JPG", "tokens": 519, "chars": 2308, "pages": {"start": 3, "end": 4}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0003", "next_id": "umontana-milestone2-validation_chunk_0005", "content_hash": "sha256:ebf8099f54e7e66a37ea33eb400f23805073411d39e6f52510a96a6eefbf6ac7"}
{"chunk_id": "umontana-milestone2-validation_chunk_0005", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 5, "text": "re 3); the final subset represents a third tree (Figure 4) \nthat was included in Figure 3, as duplication of nodes between the trees poses no issue. \n \n1 https://www.simplilearn.com/ice9/free_resources_article_thumb/phone-price.JPG \n\n \n \nPage 5 of 38 \n \n \nFigure 2 - Single Node, Single Level Decision Tree \n \n \nFigure 3 - Decision Tree with Two Nodes & Two Levels \n \n \nFigure 4 - Duplicate Decision Tree \n \n \nEach tree in the ensemble can be considered a separate ‚Äòvote' for a data point to be placed in the \n‚Äòbuy‚Äô or ‚Äòdon‚Äôt buy‚Äô labeling category. Using the above ensemble of trees, a phone > $500 could be \nbought if it had > 64 GB of storage and 4 GB of RAM. The first tree would push the needle towards \nnot buying the phone as the price is too high, but the latter two trees would push the tree towards \nbuying it. The ensemble classification results in a \"vote\" of two to one, thus the phone would be \nbought and the bias from the simple decision tree is removed. \n \nRandom forest has several other advantages that improve its performance over simplified decision \ntrees.  For example, small trees could also be trained on different datasets, further increasing \nflexibility. Also, the relative importance of different features can be analyzed by comparing the \nperformance of different tree configurations to the training dataset.   \n \nFor the purposes of this project, the random forest was trained on 39 days of Department Store A data. Nine days \nfall into the category of pre-treatment in October, 2019 (these were labeled anomalous) and 30 from \nthe category of post-treatment in May, 2020 (these were labeled normal). The features used were: \n\n \n \nPage 6 of 38 \n \n \n‚óè \nBoth median and median absolute deviation from the median (MAD) for: \n‚óã \nkW/Ton \n‚óã \nCOP \n‚óã \nEER \n‚óè \naverage and maximum outside temperature \n‚óè \nmedian and MAD capacity (TON) stratified by the number of compressors running- 1, 2, 3 or \n4 \n‚óè \nthe time 1, 2, 3, and 4 compressors are turned on \n \nThe data was filtered in two ways.  First, data points were only considered when Compressor 1 was \non.  In the data set considered, there were four compressors labeled 1 ‚Äì 4 that always came on in \norder.  Generally, Compressor 1 ran most frequently during the day; while Compressor 2 would \nperiodically kick in to provide extra cooling capacity. Compressors 3 and 4 rarely ran. Second, data \npoints were only considered when CFM was at least 100 cubic feet per minute. This helped to \nremove spikes in the calculations that occur when the machine is coming on or turning off, as CFM \ndrops to zero at these times. \n \nFigures 5 and 6 below represent examples of the decision trees the random forest model generated \nfrom the pre-treated data for the selected dates. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \nPage 7 of 38 \n \nFigure 5 - Random Forest Department Store A Example 1 \nDecision tree 1 generated on 39 days of Department Store A data (dates from 10-1-19 to 10-8-19 and 5-1-20 to 5-31-20)", "tokens": 742, "chars": 2986, "pages": {"start": 4, "end": 7}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0004", "next_id": "umontana-milestone2-validation_chunk_0006", "content_hash": "sha256:3b8b6fd5c99ea53cd4d9219e270c2da03a525b7a334728d7e008ae5c2360406b"}
{"chunk_id": "umontana-milestone2-validation_chunk_0006", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 6, "text": "rated \nfrom the pre-treated data for the selected dates. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \nPage 7 of 38 \n \nFigure 5 - Random Forest Department Store A Example 1 \nDecision tree 1 generated on 39 days of Department Store A data (dates from 10-1-19 to 10-8-19 and 5-1-20 to 5-31-20) \n \n \n \n \n \n \n \n \n\n \n \nPage 8 of 38 \n \nFigure 6 - Random Forest Department Store A Example 2 \nDecision tree 2 generated on 39 days of Department Store A data (dates from 10-1-19 to 10-8-19 and 5-1-20 to 5-31-20) \n \n \n \n \nResults \n \nTypically, another labelled data set is used to test a supervised machine learning algorithm.  This \ndata set will be used as input for the algorithm and the output will be compared to the known labels, \nallowing an accuracy percentage to be calculated. It follows that without more labelled data, it is not \npossible to conclusively test performance and produce precision accuracy measurements.  \n \nWhile lacking an understanding of true method accuracy serves as a notable limitation stemming \nfrom an absence of labeled data, the team was able to mimic a testing data set by using pre and \npost-treatment data as ‚Äòanomalies‚Äô. Once trained on the labeled data from the 39 days in October \n2019 and May 2020, the algorithm was applied to Department Store A data from June 2020 to November 2020 \nand the following days were classified as anomalies: \n- \n2020-07-12 \n- \n2020-09-01 \n- \n2020-09-09 \n\n \n \nPage 2 of 38 \n \n- \n2020-09-10 \n- \n2020-10-19 \n- \n2020-10-25 \n- \n2020-10-26 \n- \n2020-10-27 \n- \n2020-10-28 \n- \n2020-10-29 \n- \n2020-10-30 \n- \n2020-11-01 \n- \n2020-11-02 \n- \n2020-11-10 \n- \n2020-11-16 \n- \n2020-11-22 \n- \n2020-11-23 \n- \n2020-11-27 \n \nFurther investigation into the dates listed shows anomalies, so the algorithm is at least partially \nsuccessful in the identification of anomalies. For instance, consider the first anomaly July 12, 2020.  \nPlotting kW/ton for that day and the surrounding days which were not flagged as anomalous, \nproduces the results shown in Figure 7. \n \nFigure 7 - July 11th - 13th  2020 Plots \nThe left vertical axis represents kW/Ton (blue values), while the vertical axis on the right represents the number of \ncompressors running (black line). The first graph represents the 11th, the second the 12th and the third the 13th. \n \n\n \n \nPage 2 of 38 \n \nWe can see the values for July 12th (middle graph) have a visually distinct pattern and are about 100 \ntimes higher. The values are enough higher that the scale is modified automatically by the algorithm \nto show peaks of even over 200, while the 11th and the 13th both hover around 1.5 and 2.  \n \nAs a secondary result of this method, the random forest algorithm produces a list of features ranked \nby importance as they relate to the classification being performed by the algorithm. This list is shown \nin Figure 8. \n \n \nFigure 8 - Features by Importance \n \n \nThe more ‚Äòimportant‚Äô a feature is, the more it impacts whether the algorithm distinguishes a day to \nbe ‚Äònormal‚Äô or ‚Äòanomalous.  This can be useful information in selecting features to report and \ninvestigate further.", "tokens": 846, "chars": 3093, "pages": {"start": 6, "end": 10}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0005", "next_id": "umontana-milestone2-validation_chunk_0007", "content_hash": "sha256:f8a34d3c06d209cbf6d9a243ce4e153879e40d35552304f4b7534f1a2e9346fc"}
{"chunk_id": "umontana-milestone2-validation_chunk_0007", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 7, "text": "the algorithm. This list is shown \nin Figure 8. \n \n \nFigure 8 - Features by Importance \n \n \nThe more ‚Äòimportant‚Äô a feature is, the more it impacts whether the algorithm distinguishes a day to \nbe ‚Äònormal‚Äô or ‚Äòanomalous.  This can be useful information in selecting features to report and \ninvestigate further. \n \n \n \n\n \n \nPage 3 of 38 \n \nConclusion and Possible Uses \n \nIn conclusion, this method shows promise but needs more labeled data to fully evaluate. If \nPowerTron were able to provide a labeled data set with clear indicators of instances where an \nanomaly they were interested in tracking occurred, this algorithm could be used to investigate \nanomalies non-labeled datasets from other months of operation.  \n \nFurthermore, when used in combination with other statistical models, it may be possible to create a \ndeeper understanding of ‚Äòwarning signs‚Äô that typify pre-breakdown behavior in a device. With this \nunderstanding, a labeled dataset could be created that trains the algorithm to detect pre-breakdown \nbehavior and flag this occurrence in an active manner. \n \nAdditionally, impacts of decisions made by building engineers and managements could be tracked. \nThe training dataset would need labels for variable measurements known to be an outcome of a \nuser behavior i.e. When building management decides to do x action, the COP/EER/etc do y. The \nalgorithm would then identify and track these incidents over time. \n \nObvious limitations to this method come rooted in the fact that it is a supervised learning algorithm \nrequiring a training set of labeled data to produce results, thus requiring knowledge of what \ncharacterizes an anomaly prior to running the algorithm. Further findings and uses could possibly be \ngenerated via additional labeled data or data annotations describing decisions made by building \nengineers, mechanical issues, and more. \n \nDetecting outliers ‚Äì An Analysis of Variability:   \n \nMedian Absolute Deviation from the Median (MAD) Method Description \n \nThe Median Absolute Deviation from the Median (MAD) is a measure of variability. This idea of MAD \nwas introduced in a talk by the head data scientist at Datadog.2 The method is more simply \nimplemented than the interquartile range and is less impacted by outliers by using the median. While \nthe values in any data set are expected to vary, if they change ‚Äòtoo much‚Äô it should trigger further \ninvestigation within an analysis. Defining ‚Äòtoo much change‚Äô within a dataset is commonly called \n\"thresholding.\"  Thresholds are often set manually, but the team wanted to investigate if these \nthresholds could be automatically applied. If a machine could automatically and actively apply \nthresholds to a dataset characterizing machine behavior, the use cases would be extensive. \n \nThe milestone 1 report for this project discussed using the interquartile range (IQR) to identify \noutliers.  Presented below is a similar method using a different statistic: the median absolute \ndeviation from the median. \n \nMAD is calculated as follows: \n \nFind the median of the dataset \nD = { 1, 2, 3, 4, 5, 6, 100} \n \n2 Detecting outliers and anomalies in real-time at Datadog, Homin Lee. Presented at OSCON, Austin \nTexas, May 16-19,  2016. https://www.youtube.com/watch?v=mG4ZpEhRKHA", "tokens": 711, "chars": 3269, "pages": {"start": 10, "end": 11}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0006", "next_id": "umontana-milestone2-validation_chunk_0008", "content_hash": "sha256:c5224f2018036e2eefb287302bcd5af110c839bd7eb890a248b37acbd2361632"}
{"chunk_id": "umontana-milestone2-validation_chunk_0008", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 8, "text": "t statistic: the median absolute \ndeviation from the median. \n \nMAD is calculated as follows: \n \nFind the median of the dataset \nD = { 1, 2, 3, 4, 5, 6, 100} \n \n2 Detecting outliers and anomalies in real-time at Datadog, Homin Lee. Presented at OSCON, Austin \nTexas, May 16-19,  2016. https://www.youtube.com/watch?v=mG4ZpEhRKHA \n\n \n \nPage 4 of 38 \n \n \nmedian =4 \n \nFind the deviations from the median \ndeviations  =  \n \n{ -3, -2, -1, 0, 1, 2, 96 } \n \nTake the absolute value \nabsolute deviations  =  \n \n{ 3, 2, 1, 0, 1, 2, 96 } \n \nFind the median of the absolute \nsorted absolute deviations =  \ndeviations \n{ 0, 1, 1, 2, 2, 3, 96 } \n \nMAD = 2 \n \nUsing the MAD, thresholds can be set:   median +/- MAD * tolerance factor. In the above data set, \nusing a tolerance factor of 3, our thresholds would be set at -2 and 10 (4 +/- 2*3).  Thus, any values \nbelow -2 or above 10 would be considered outliers. \n \nResults and Possible Use Cases \n \nConsider an example from the Macy‚Äôs data set during the month of May 2020. The data was filtered \nso that only values where compressor 1 was on and cfm > 100 were considered. (Figures 9 and \n10).  The median kW/ton in the May 2020 dataset was 1.09, while the MAD was 0.19. On May 4, \n2020, 1.85% of the data were outliers (Figure 9), but on May 5th, 52.98% of the values were \noutliers (Figure 10).  This breach in the threshold could indicate a malfunction and warrant further \ninvestigation.   \n \nFigure 9 - kW/Ton Outliers in May 2020 (Department Store A) \nOutliers in the kW/Ton measurements are marked with a red dot; outliers in the CFM measurements are \nnotated with a green dot, while blue marks standard kW/Ton measurements over time.", "tokens": 518, "chars": 1681, "pages": {"start": 11, "end": 12}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0007", "next_id": "umontana-milestone2-validation_chunk_0009", "content_hash": "sha256:af8f68e9cb614bcd082b6222794f6163b35a2cc4f271ed6f69e9e2d3f6ed1e44"}
{"chunk_id": "umontana-milestone2-validation_chunk_0009", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 9, "text": "e kW/Ton measurements are marked with a red dot; outliers in the CFM measurements are \nnotated with a green dot, while blue marks standard kW/Ton measurements over time. \n \n\n \n \nPage 5 of 38 \n \nFigure 10 - kW/Ton Outliers in May 2020 (Department Store A) \n \n \nTo apply this method, one must choose the tolerance factor, the period of time for which the median \nis calculated, and how many outliers during a period would trigger an alert. For this exploration, the \nteam selected a tolerance factor of 3, calculated the median in monthly increments, and specifically \ntriggered the alert when the amount of outliers contained in the dataset amounted to 30% or more of \nthe data within the set.  \n \nDepending on the expected regularity of the data, one could ‚Äòzoom in‚Äô by reducing the threshold for \npercentage of the set classified to be outliers. If changes occur that may be of interest within smaller \nperiods of time, the time increment for median calculation could be modified to ensure changes \nshifts in the median over time are not lost. To understand the best measurement conditions, selected \npresets for the algorithm should be tested via experiments on historical data to determine what \nprovides the most pointed outcomes.  For example, median could be a running value calculated \ncontinuously, or could be calculated only on data taken during a matching outdoor air temperature.   \n \nThis model has potential to detect possible mechanical failure in real time, however more historical \ndata would be needed to examine trends and set appropriate thresholds based on understanding of \nthe data in previously recorded mechanical failure or malfunction. This use case may also need \nfurther variables not being calculated at this time, such as pressure, that may need more sensor \nequipment. Historical data would need to be labeled so results can be understood against real \nrecord of mechanical failure. \n \nAnother key statistic that would be worth pulling on an ongoing basis to better assess this \nmethodology would be a running measure of the spread of the data. \n \nThis method has real potential use cases within its ability to be connected with an application and \nactively flagging the engineer or device user when an amount above the threshold of outliers is \ndetected.", "tokens": 459, "chars": 2287, "pages": {"start": 12, "end": 13}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0008", "next_id": "umontana-milestone2-validation_chunk_0010", "content_hash": "sha256:af7ade01bbe2d9fc9ad007e8446880be0bd3189697555f690fa5e651f26e9a07"}
{"chunk_id": "umontana-milestone2-validation_chunk_0010", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 10, "text": "pread of the data. \n \nThis method has real potential use cases within its ability to be connected with an application and \nactively flagging the engineer or device user when an amount above the threshold of outliers is \ndetected. \n \n \n\n \n \nPage 6 of 38 \n \nAgglomerative Hierarchical Clustering \n \nOverview \n \nHierarchical clustering is a type of clustering algorithm that takes input data and categorizes it into \ngroups known as clusters. To cluster data, the algorithm‚Äôs input data is assembled as objects, each \nobject being characterized by a series of variables. In clustering time series, measurement variables \nare selected as features to represent a particular measurement period, each measurement period \nrepresenting an object to be clustered by the algorithm, grouped according to the similarity of its \nfeatures against other objects. \n \nFor this exploration, pre-treatment of the data involved selecting an amount of time and then \nextracting a handful of features to characterize that period of time. Ideally, the clusters produced by \nthe algorithm would classify the objects and produce groupings to show which objects were \nanomalies and which represented those objects with standard measurements.  \n \nThe team decided to build a data set of days (objects) that each had seven variable measurements \nthe algorithm then used to compare one day to the next. Characterizing each day were kW/Ton, \nmedian COP, capacity in tons BTU when compressor 1 was running, capacity in tons BTU when \nboth compressors 1 and 2 were running, maximum outside air temperature, the time 1 compressor \nwas active, and the time when both compressors 1 and 2 were active. Thus, each day was defined \nas an object by the above variables and then compared to the other days (objects) in the data set \nbased on the similarities and differences between the measured features of the given object. \n \nFor hierarchical clustering, the input data consisted of 8 days from the Macy‚Äôs data set in October \n2019 and 30 days from the Macy‚Äôs data set in May 2020. The October data stood to characterize \ndevice performance prior to PowerTron‚Äôs treatment of the device (pre-treatment) and the days in \nMay portray device performance post-treatment. These days were selected due to the implied \ndifference in the efficiency of the device pre and post-treatment that should be manifested in the \nfeature measurements collected for these days. The algorithm should cluster the days accordingly \nand the success of the method‚Äôs output can be compared to the expectation that these sets of days \nare different and should be classified as such. \n \nFinally, an essential step for the pre-treatment of data is the normalizing of the input data set. If each \nrow of data represents a day and the columns are the characteristics measured and recorded for \nthat day, the features can be normalized to produce a percentage based on all other measurements \nfor that feature. The differences between raw and normalized data can be seen in Figures 11 and 12 \nbelow. \n \nFigure 11 - Raw Department Store A Data", "tokens": 623, "chars": 3072, "pages": {"start": 13, "end": 14}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0009", "next_id": "umontana-milestone2-validation_chunk_0011", "content_hash": "sha256:8b268529af6a248ffc5e75a9aa115845a7b3d59893b34ffd47e2d69ec622c104"}
{"chunk_id": "umontana-milestone2-validation_chunk_0011", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 11, "text": "e columns are the characteristics measured and recorded for \nthat day, the features can be normalized to produce a percentage based on all other measurements \nfor that feature. The differences between raw and normalized data can be seen in Figures 11 and 12 \nbelow. \n \nFigure 11 - Raw Department Store A Data \n \n\n \n \nPage 7 of 38 \n \n \nFigure 12 - Normalized Data \n \n \nFigure 11 represents the data prior to normalizing the measurements, while Figure 12 shows \nthe normalized characteristics. Normalization shifts and rescales the data points within a field \n(e.g. kW/Ton or Median COP) so they end in a scale from 0 to 1 and thus are more similar \nacross the dataset3. The normalized point is generated by evaluating all data points within each \nfeature to find a range, then taking the raw data point, subtracting the minimum data point within \nthe field and dividing the result by the range for the field.  \n \nùëã‡Ø°‡Ø¢‡Ø•‡Ø†‡Øî‡Øü‡Øú‡Ø≠‡Øò‡Øó= (ùëã‡Ø•‡Øî‡Ø™‚àíùëã‡Ø†‡Øú‡Ø°‡Øú‡Ø†‡Ø®‡Ø†)/(ùëã‡Ø†‡Øî‡Ø´‡Øú‡Ø†‡Ø®‡Ø†‚àíùëã‡Ø†‡Øú‡Ø°‡Øú‡Ø†‡Ø®‡Ø†) \n \nThe normalized data points illustrate the raw measurements in a controlled fashion, facilitating \nthe visualization and comparison of the points within the set. This is important when building \ncomplex comparisons, such as the distance matrix seen in the next sub-section. \n \n \nBasis of the Algorithm \n \nThe process used by the hierarchical clustering algorithm to compare and then cluster the data is \nwhat makes each algorithm unique. Hierarchical clustering builds a hierarchy based on the distance \nof each object from other objects in the multi-variate space. Consider mapping each object in a \nspace with many dimensions (in this case, each day in a space with 7 dimensions) and then \nanalyzing the distance between each point. \n \nIt is helpful to start by imagining a generic data set where each object is characterized by only two \nmeasurements i.e. each object has two features used to compare it to other objects in the set. The \nalgorithm begins by creating a distance matrix, also referred to as a dissimilarity matrix, to calculate \nthe distance between each object. \n \nAn example of the distance matrix produced for this generic, two-feature data set is shown in Figure \n13 below4.  \n \n \n3 https://towardsdatascience.com/normalization-vs-standardization-explained-209e84d0f81e  \n4 https://www.displayr.com/what-is-a-distance-matrix/", "tokens": 571, "chars": 2310, "pages": {"start": 14, "end": 15}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0010", "next_id": "umontana-milestone2-validation_chunk_0012", "content_hash": "sha256:2d0f84145799d3e4f634ac70f32def08f35ee38196eea4c36a2965d383ad0089"}
{"chunk_id": "umontana-milestone2-validation_chunk_0012", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 12, "text": "x produced for this generic, two-feature data set is shown in Figure \n13 below4.  \n \n \n3 https://towardsdatascience.com/normalization-vs-standardization-explained-209e84d0f81e  \n4 https://www.displayr.com/what-is-a-distance-matrix/  \n\n \n \nPage 8 of 38 \n \nFigure 13 - Generic Distance Matrix Example\n \nThe way that distance is calculated for each object (A-F in the figure above) is by using the \nPythagorean theorem. For instance, the distance between objects A and C is calculated by the \nfollowing formula... \n‡∂•(51 ‚àí9)‡¨∂+ (28 ‚àí49)‡¨∂= 46.95743.. . ‚âà47 \n \nThis example helps to understand the process of creating a distance matrix in a simplified manner. \nHowever, in the analysis of the given data set, each object is characterized by 7 variables. In this \ncase, distance is instead calculated using the Euclidean distance formula so all variables can be \nconsidered, as opposed to the two-variable distance produced by the Pythagorean theorem. The \nresulting distance matrix can be found in the ‚ÄòResults‚Äô sub-section for hierarchical clustering. \n \nOnce the algorithm has produced a distance matrix, the distances are compared in a hierarchical \nfashion, first grouping the two data points closest together to form a cluster. The assumption that \neach data point is initially treated as an independent cluster and then iteratively grouped together \nusing the distance matrix defines this method to be agglomerative, as opposed to divisive5. Once a \nlink has been created between the closest two points, the next closest distance is determined \nbetween the two-point cluster and the other data points, working to find the shortest distance. After \nthe next shortest distance is determined, a link is created. This process occurs iteratively and \nproduces a visualization of the hierarchy mapping the distance between points known as a \ndendrogram. The dendrogram shows linkages across all data points and is eventually used to \ndetermine the input for number of clusters when commanding the algorithm to produce cluster \nassignments. \n \nAfter the number of clusters has been determined by the dendrogram, the algorithm can be run set \nto the number of clusters determined to receive an output depicting the cluster assignments for all \ninput objects e.g. Day x belongs in cluster y based on its relationship to all other data points. To \nbetter understand the cluster assignments and place them in context of the selected features, the \nobjects can be visualized with a scatter plot. Each data point on the plot represents a day as it falls \nwithin the framework of the selected features ‚Äì while the color of the points in the plot can be \nmodulated to represent what cluster the point was assigned to. \n \n5 https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-\nc6e8243758ec  \n\n \n \nPage 9 of 38 \n \n \nScatter plots and a dendrogram for the Macy‚Äôs data are included in the following results section.  \n \nResults, Conclusions, and Possible Uses \n \nTo begin, the team used the Euclidean distance and created a distance matrix between all \npoints within the normalized data set. When calculated a 38 x 38 distance matrix was produced \nshowing the distances between all 38 days assessed. A portion of this matrix can be seen in \nFigure 14. \n \nFigure 14 - Department Store A Oct 2019/May 2020 Dissimilarity Matrix \n \n \nFrom the matrix, the dendrogram is then produced, starting by grouping the closest two days, then \nadding the 3rd closest and so on until the iterative process has analyzed and assigned a cluster to \neach day. Figure 15 shows the dendrogram produced by the algorithm package when the October \n2019/May 2020 dataset is inputted.", "tokens": 808, "chars": 3679, "pages": {"start": 15, "end": 17}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0011", "next_id": "umontana-milestone2-validation_chunk_0013", "content_hash": "sha256:d2698ece078a657a0b907163ee1851f8ea626bc3acd0aa3ed5ad8101f2112fa5"}
{"chunk_id": "umontana-milestone2-validation_chunk_0013", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 13, "text": "2019/May 2020 Dissimilarity Matrix \n \n \nFrom the matrix, the dendrogram is then produced, starting by grouping the closest two days, then \nadding the 3rd closest and so on until the iterative process has analyzed and assigned a cluster to \neach day. Figure 15 shows the dendrogram produced by the algorithm package when the October \n2019/May 2020 dataset is inputted. \n \n\n \n \nPage 10 of 38 \n \nFigure 15 - Dendrogram of October 2019/May 2020 Data \n \n \nThe y-axis in figure 15 is used to measure the height of the line connecting each object notated on \nthe x-axis. The taller the line representing the linkage between objects, the further apart those \nobjects are calculated to be. The colors within the dendrogram indicate the cluster assignments and \nsuggest that 4 clusters should be used when assigning clusters within the data. \n \nAt this point, all the inputs have been collected to run the hierarchical clustering algorithm that \nproduces a matrix assigning a cluster (cluster 0, 1, 2 or 3) to all 38 days included in the dataset. The \noutput matrix is seen in figure 16.  \n \nFigure 16 - Cluster Assignments within October 2019/May 2020 Dataset \n \nThe highlighted days in figure 16 are the first 8 days listed in the dataset i.e. the days from October \n2019, the others listed are days in May 2020. There is some overlap in the measured device \nbehavior on days 7 and 8, as they fell into the ‚Äò0‚Äô cluster where the majority of days in that cluster are \ndays from May. This overlap is minimal as the remaining days from October were recognized as a \ncluster of their own, not containing any data sets from May 2020. \n \n\n \n \nPage 11 of 38 \n \nThere were also two outliers identified within the days from May, clustered within clusters 2 and 3. \nThese are seen to be outliers as they are single days that were dissimilar enough from the points in \nclusters 0 and 1 that the algorithm chose to classify them in their own categories entirely. \n \nOverall, the result produced is reasonably successful, as the majority of days from the pre-treatment \ndata were classified separately from the post-treatment data. This aligns with the expectation that \nthe behavior of the device would be different between pre and post PermaFrost treatment \nimplemented by PowerTron. \n \nTo examine possibilities even further, the team decided to look at several scatter plots of the data, \nwhere the clusters have been identified. The two most interesting plots produced that illustrate not \nonly the behavior of the clusters, but also the success of PowerTron‚Äôs treatment are shown below in \nfigures 17 and 18. \nFigure 17 - COP Against Outside Air Temperature  \nCOP is measured on the y-axis; Outside air temperature is measured on the x-axis; Clusters are represented by the \ncolor of the points, which each represent a day; Note ‚Äì The axes remain between 0 and 1 as the data has been \nnormalized", "tokens": 653, "chars": 2879, "pages": {"start": 17, "end": 19}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0012", "next_id": "umontana-milestone2-validation_chunk_0014", "content_hash": "sha256:753c2c86062e02f6478c82bdb31ed1b7e1d5a3d72e9d63da1434c2750eb78c3f"}
{"chunk_id": "umontana-milestone2-validation_chunk_0014", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 14, "text": "8. \nFigure 17 - COP Against Outside Air Temperature  \nCOP is measured on the y-axis; Outside air temperature is measured on the x-axis; Clusters are represented by the \ncolor of the points, which each represent a day; Note ‚Äì The axes remain between 0 and 1 as the data has been \nnormalized \n \n\n \n \nPage 12 of 38 \n \nFigure 18 - kW/Ton against Outside Air Temperature \nkW/Ton are measured on the y-axis; outdoor air temperature is measured on the x-axis; each point represents a day \nin the data set and the color of the point indicates the assigned cluster \n \n \nFrom Figure 17, a conclusion can be reached regarding the COP measurements for the clustered \ndata points in yellow. Simply stated, the COP represents the heat removed by the device and thus a \nhigher COP is better when the device is on. The yellow data points from October (pre-treatment) \nmanifest a low COP even when the temperature outside is measured near it‚Äôs highest i.e. the device \nis not succeeding in removing large amounts of heat at high temperatures pre-treatment. Post-\ntreatment the COP increases dramatically for the majority of the data points, with the exception of a \nhandful of outliers. This means that the algorithm proves the effectiveness and confirms the \nexpected positive impact that PowerTron‚Äôs treatment has on the device. The effect is sure enough \nthat even an algorithm that is fed unlabeled data picks up on the trend. \n \nFigure 18 manifests a similar proof of expected treatment outcomes in the kW/Ton measurement. If \na treatment were intended to improve the efficiency of a device, one would expect the amount of \nelectricity used to supply a given amount of cooling to decrease. This is seen as the kW/Ton for \nthose pre-treatment October days is clustered in the upper right hand of Figure 18 where the \ngrouping of cluster 1, yellow dots are located. This indicates the majority of data post-treatment \nshows less energy being used to provide the same amount cooling, despite high temperature \nreadings. The algorithm, despite not knowing which days were pre and post-treatment, has identified \nthe difference in pre and post-treatment data and once again proves the efficacy of PowerTron‚Äôs \nPermaFrost treatment on the Macy‚Äôs unit. \nAn existing use of such findings is to use the data in advertising or marketing the treatment to \nprospective clients. This is evident from a high-level exploration, but also overlooks certain potential", "tokens": 520, "chars": 2437, "pages": {"start": 19, "end": 20}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0013", "next_id": "umontana-milestone2-validation_chunk_0015", "content_hash": "sha256:4f0859be111249e3e03eab861b90e5502b5185c47ce86134cdd7ee2b5cacdc0c"}
{"chunk_id": "umontana-milestone2-validation_chunk_0015", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 15, "text": "PermaFrost treatment on the Macy‚Äôs unit. \nAn existing use of such findings is to use the data in advertising or marketing the treatment to \nprospective clients. This is evident from a high-level exploration, but also overlooks certain potential \n\n \n \nPage 13 of 38 \n \nuses for this method in other scenarios. For instance, this method could be used to examine source \nmeasurements (e.g. the wet bulb measurement and temp 1 & 2) and then compare the resulting \ncluster to the expectations set in the laws of physics and defined by the psychrometric chart. Given \nthe nature of physical phenomenon, one would expect the relationships between certain source \nreadings to fall within a particular cluster. If, when actively feeding the algorithm data day to day, the \ndata begins to shift enough that it eventually triggers the creation of a new cluster by the algorithm, it \ncould possibly point to a future mechanical failure or problem. \n \nWith limited success comes the need for further exploration of the algorithm‚Äôs performance on \ndifferent data sets and modified algorithm settings to shift the model and develop a deeper \nunderstanding. This initial examination deemed the algorithm to be successful enough that teams \ncould explore the possibilities more thoroughly in the future. \n \nArchetypal Analysis \n \nArchetype Background \n \nArchetypal Analysis (AA) is a tool for the study of large data sets, used in both data compression, \nand the analysis of spatio-temporal patterns in data (Cutler and Stone, 1997; Stone and Cutler, \n1996; Bauckhage, 2014; Vinue et al., 2015). Recently, with the advent of greater computational \npower, it has been used in a variety of applications in the analysis of ‚ÄúBig Data\". For instance, it has \nbeen used to analyze weather, climate and precipitation patterns (Hannachi and Trendafilov, 2017; \nSteinschneider and Lall, 2015; Su et al., 2017). A probabilistic framework for archetypes is \ndeveloped in (Seth and Eugster, 2016). Its application to machine learning appears in M√∏rup and \nHansen (2012). It is also used in biomedical and industrial engineering (Epifanio et al., 2013; \nTh√∏gersen et al., 2013), and in the analysis of terrorist events (Lundberg, 2019).  \n \nArchetypal Analysis was introduced by Cutler and Breiman in 1994 as variant of principal component \nanalysis (PCA) that could capture ‚Äôarchetypal patterns‚Äô in the data (Cutler and Breiman, 1994). \nThrough AA, each time-based observation is represented as a convex combination of a limited \nnumber of points, called ‚Äôarchetypes‚Äô or ‚Äôpure types‚Äô, which may or may not be observed. These \ninfluential data points best describe the exterior surface of the original data set, and as convex \ncombinations of the data points themselves, resemble the observations. \n \nAA provides a number of advantages over commonly used techniques for data compression and \nclustering, such as Principal Component Analysis, or PCA (Pearson, 1901; Abdi and Williams, 2010) \nor k-mean clustering (MacQueen, 1967). PCA can lead to a complex representation of the data and \nis restricted by orthogonality, so meaningful features may not be discovered. Clustering approaches \nprovide easy interpretation, but tend to lack modeling flexibility, with each observation grouped in \nonly one cluster, no in between or intermediate groupings are allowed. In contrast, with AA, each \nobserved data point is either classified to its closest archetype (single), or it is associated with two or \nmore archetypes (mixed).", "tokens": 789, "chars": 3493, "pages": {"start": 20, "end": 21}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0014", "next_id": "umontana-milestone2-validation_chunk_0016", "content_hash": "sha256:47393ece82df9a883c443d19201654e6886b8d8a5e9d0978111009cd4837a032"}
{"chunk_id": "umontana-milestone2-validation_chunk_0016", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 16, "text": "ing approaches \nprovide easy interpretation, but tend to lack modeling flexibility, with each observation grouped in \nonly one cluster, no in between or intermediate groupings are allowed. In contrast, with AA, each \nobserved data point is either classified to its closest archetype (single), or it is associated with two or \nmore archetypes (mixed). Therefore, AA combines the virtue of both methods; it is easy to interpret \nand by allowing intermediates, provides more flexibility than clustering. \n \nMathematical Formulation \n \n\n \n \nPage 14 of 38 \n \nAppendix A shows the mathematical formulation for the archetypes in detail. \n \nResults \n \nThe team applied archetypal analysis to the summary data set produced by [Faculty Advisor A], \nwhere pre-treatment is described in the hierarchical clustering section.  Each data point represents \none ‚Äúcooling day‚Äù and contains some number of summary measurements for that day. To test the \nmethod, we combined the October 2019 data set with the May 2020 set, to see if archetypal analysis \ncould distinguish pre- and post- treatment operation of the machine.  We chose a subset of the full \nsummary data set, considering only the median values, and pruning variables that have a direct and \nobvious relation with each other.  For instance, the max OAtemp and the median OAtemp are \nplotted against each other in Figure 19.  A linear regression shows that they are the same variable, \nup to a vertical shift.  Therefore, one can be excluded without loss of generality.  We chose to retain \nthe maximum daily temperature.  Similarly, EER and COP are also linearly related, as they are \nmeasurements of the same thing, in different units. See Figure 20.  We chose the median COP for \nour data set.  We then selected Kw/ton, which is inversely related to COP, generally speaking, but \nvariations of this dependence could indicate varying operating conditions.  Also included are median \ntotal capacity in BTU tons when compressor 1 is on, and when compressors 1 and 2 are on.  Finally, \nthe total time that compressor 1 and compressor 2 are on during the day were included.  \nCompressor 3 and 4 are not always turned on each day, subject to control by the machine operator, \nbut could be included in another round of analysis, as the total capacity will be effected by this.   \n \nFigure 19 - Max OAtemp vs. Median OAtemp  \nA linear relationship is shown by the line in yellow and described by the formula listed in the upper left corner of the \nfigure \n \n \n\n \n \nPage 15 of 38 \n \nFigure 20 - Median EER plotted against Median COP  \nA linear relationship (with few outliers) is apparent. \n \nIn summary, the variables that make up each data point are:  \n \n1. Median Kw/ton - (kwtons) \n2. Median COP - (med COP) \n3. Median capacity in BTU tons when compressor 1 is on - (comp1Ton) \n4. Median capacity in BTU tons when compressor 1 and 2 are on - (comp2Ton) \n5. Max outside air temp in degrees F - (max temp) \n6. Amount of time compressor 1 is on in seconds - (time1) \n7. Amount of time compressor 2 is on in seconds - (time2) \n \nDates of the 8 points from October 2019  are:  10/01-10/06 and 10/08-10/09. \n \nDates of the 30 points from May 2020 are: 05/01-05/30. \n \nFigure 21 shows time series plots of the data.", "tokens": 787, "chars": 3249, "pages": {"start": 21, "end": 23}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0015", "next_id": "umontana-milestone2-validation_chunk_0017", "content_hash": "sha256:2b76ed2d166b8c431fbc398f0b23652fcc374e26430bffcfdd5cbab12c56118a"}
{"chunk_id": "umontana-milestone2-validation_chunk_0017", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 17, "text": "grees F - (max temp) \n6. Amount of time compressor 1 is on in seconds - (time1) \n7. Amount of time compressor 2 is on in seconds - (time2) \n \nDates of the 8 points from October 2019  are:  10/01-10/06 and 10/08-10/09. \n \nDates of the 30 points from May 2020 are: 05/01-05/30. \n \nFigure 21 shows time series plots of the data.  \n\n \n \nPage 16 of 38 \n \nFigure 21 - Time Series Plot of Each Variable  \nDates are on the x-axis, units of the variable on the y-axis. Note that COP is unitless \n \nPrior to performing archetypal analysis, the 38 points are normalized in each dimension separately, \nby dividing each measurement by the maximum value of the measurement in the data set.  Thus, \nthe variables all ranges between 0 and 1, with fractions representing percentages, e.g., 0.5 is 50% of \nthe max value achieved by that variable in the data set.   \nThe first step in applying archetypal analysis is to compute the total error in representing the data in \nterms of the archetypes, in this case it is the residual sum of squares, or RSS.  The RSS is \ncomputed for each set of archetypes and decreases as a larger number of archetypes is taken.  See \nFigure 22.   \n\n \n \nPage 17 of 38 \n \nFigure 22 - Residual Sum of Squares (RSS) vs. Number of Archetypes for the Oct2019/May2020 \nData Set  \nThe algorithm creates a set of N archetypes sequentially, each accompanied with the total error (RSS) in \nrepresenting the data with that set of archetypes \n \nWith only one archetype computed the error is roughly 118.5, but drops quickly when a two \narchetype set is computed.  We chose to truncate at 3 archetypes; adding any more does not \nsignificantly affect the accuracy of the data reconstruction.  Archetypal analysis thus reduces the \ndata set from 7 dimensions to 3, and the archetypes themselves are representative of typical \n‚Äúcooling days‚Äù.   A bar chart of the variables in each archetype is shown in figure 23. All three have \nsimilar large max temperatures, but archetype 2 is a more efficient day, as the median COP is larger \nand the Kw/ton is smaller for it than the other two. Also, compressors 1 and 2 have large capacity \noutput, and both are on for a similar amount of time.  Archetypes 1 and 3 are less efficient days, with \nhigher Kw/ton and lower COP.  Archetype 1 represents a day where compressor 2 is not turned on, \nand archetype 3 will capture less efficient days when both compressors are on. Each data point can \nbe reconstructed by a convex combination of the 3 archetypes, and the mixture coefficients (the \nalphas) for each is plotted in figure 24.", "tokens": 652, "chars": 2569, "pages": {"start": 23, "end": 25}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0016", "next_id": "umontana-milestone2-validation_chunk_0018", "content_hash": "sha256:9376676f6d7e711213fdf30ec3f1046b52b5e60904a356b18ff4349f20177101"}
{"chunk_id": "umontana-milestone2-validation_chunk_0018", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 18, "text": "is not turned on, \nand archetype 3 will capture less efficient days when both compressors are on. Each data point can \nbe reconstructed by a convex combination of the 3 archetypes, and the mixture coefficients (the \nalphas) for each is plotted in figure 24.   \n \n\n \n \nPage 18 of 38 \n \nFigure 23 - Bar Chart Representation of the 3 Archetype Set  \nEach archetype is like a data point from the set and made up of magnitudes of each of the seven measurement \nvariables (betas). \n \nFigure 24 - Mixture Coefficients (alphas) for each data point, 1-38 \nThe vertical line runs through Oct. 8th, the last day of the Oct. 2019 data set. From there on, the data points (days) \nare in May. Thus, the line divides pre-treatment days and post-treatment days \n \nThe mixture coefficient time series in figure 24 illustrate that archetype 3, and then a mixture of 3 \nand 1, represent the pre-treatment days. This is consistent with the properties of these two \narchetypes, and both are less efficient than archetype 2. The difference is if compressor 2 is on or \n\n \n \nPage 19 of 38 \n \nnot. May days are represented largely by archetypes 1 and 2, a more efficient archetype combined \nwith another where compressor 2 is turned off. A 4-archetype set would most likely split up the \nbehavior better, but would not generate a significant reduction in the reconstruction error.   \n \nFigure 25 - 3D Plot of Data Points as Represented by 3 Alpha's  \nThe simplex is enclosed by the triangle. Red points are from May 2020, blue from October 2019. \n \nFrom this, it is evident that the pre-treatments are less efficient than the post-treatments, as would \nbe hoped.  It coincides with the findings of PowerTron in its Department Store A PermaFrost NMR Confirmatory \nTest report (August 18, 2020). \n \nAnother way to represent the data is a 3D plot of the mixture coefficients.  These will lie on a simplex \n(represented by the triangular face in Figure 25) because the sum of the alpha‚Äôs for each data point \nwill be 1, and the alpha‚Äôs themselves range between 0 and 1.  Thus, each data point (cooling day) is \nrepresented (approximately) by a convex combination of alpha‚Äôs and can be plotted on a 3-D \nsimplex in the case of 3 archetypes.   The first 6 days of Oct. 2019 lie in a cluster near archetype 3.  \nThe last 2 days are closer to archetype 1.  This archetype captures the lower temperature days on \nOct. 7th and 8th (see figure 21).  The data points in May lie near the simplex edge between archetype \n1 and 2, and thus are well represented by a convex combination of the two.  This allows for a \nvariation in efficiency and whether or not compressor 2 is on.", "tokens": 661, "chars": 2640, "pages": {"start": 25, "end": 27}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0017", "next_id": "umontana-milestone2-validation_chunk_0019", "content_hash": "sha256:bda0329644e852979fe589db434a8624fcb500c553b8b343a3c9657e56664dc8"}
{"chunk_id": "umontana-milestone2-validation_chunk_0019", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 19, "text": "on \nOct. 7th and 8th (see figure 21).  The data points in May lie near the simplex edge between archetype \n1 and 2, and thus are well represented by a convex combination of the two.  This allows for a \nvariation in efficiency and whether or not compressor 2 is on.   \n  \n \n \n \n\n \n \nPage 20 of 38 \n \nConclusion and Possible Use Case \n \nWe have shown that archetypal analysis applied the summary variables chosen can differentiate \nbetween pre and post treatment days.  It also shows sensitivity to variation in outdoor temperature \nand whether compressor 2 is on.  We speculate that it could capture a gradual change in operating \nconditions through observations of the alpha‚Äôs.  If, over time, the data point drift from one condition \nto another, the representation in terms of archetypes would shift from all in the first archetypal \ncondition, to all in the other, with points in between lying on the simplex boundary between the two.  \nAnother possible use would be the detection of anomalies.  First, a set of archetypes representing \nthe usual operation of the machine is created, using a training set of the summary variables.  \nAdditional days could be tested by projecting their data points onto the archetypal set (an \noptimization problem with a nonlinear constraint).  If the error jumps up for the new data point, it \ncould be anomalous.   The daily calculation on board the machine would be the creation of the \nsummary variables, and a projection onto the archetypal set.  If an anomaly is detected, further \nclassification that day‚Äôs variables could be done to identify the source of the anomaly.   \n \nLack of Test Data: An attempt to manually produce anomaly ridden data sets. \n \nA theme throughout the testing of various methods was the lack of annotated data that would allow \nthe team to test each method against known anomalies in the data stemming from mechanical \nfailure. As such, an attempt was made to manually produce a training set of data and then plant \nknown anomalies or failures within the set.  \n \nThe following section describes the methodology used to develop the simulated set for \ndocumentation purposes only. The results from testing the methods described in previous sections \non the simulated set were inconclusive. Further examination would be needed to determine the \nfailure of the simulated data produced and modify the method accordingly. \n \nBasis for Manual Calculations \n \nFrom 25 fields of source data, 12 variables were used to produce 8 interim calculations, which then \nwere used to calculate 5 key performance indicators related to the efficiency of the device. Manual \nreplication of calculations was achieved via exploration of the relationships between all \nmeasurements. Figure 26 outlines the relationship between all variables and was also included in \nthe team‚Äôs milestone 1 report. \n\n \n \nPage 21 of 38 \n \nFigure 26 - Matrix Identifying Relationships Between Variables  \nVariables marked in red font were not used in the calculation of the efficiency KPIs \n \nWithin the source data for Department Store A, 15 months of complete or partial data was presented, captured in \n1 second intervals. The result of this measurement frequency was an extremely large dataset only \nrepresentative of a single year of data potentially impacted by seasonal weather variances.  \nMethodology for Producing Simulated Data \nAfter examining and constructing the above table representing the relationship between source and \ncalculated variables, development of simulated data began. Initially to ensure accurate construction \nof a dataset, the period and timing of source data, average relative humidity and high/low \ntemperatures for Denton, Texas were gathered and used to create relevant values for the 15 month \nperiod in the source data.  Average monthly low temperature was set for 6:00 AM on the 15th day of", "tokens": 782, "chars": 3854, "pages": {"start": 27, "end": 29}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0018", "next_id": "umontana-milestone2-validation_chunk_0020", "content_hash": "sha256:926c4cf07f45f479b16efc469e72e2c2e950c1fc2e04c9f2b2566d1d688e7a61"}
{"chunk_id": "umontana-milestone2-validation_chunk_0020", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 20, "text": "ulated variables, development of simulated data began. Initially to ensure accurate construction \nof a dataset, the period and timing of source data, average relative humidity and high/low \ntemperatures for Denton, Texas were gathered and used to create relevant values for the 15 month \nperiod in the source data.  Average monthly low temperature was set for 6:00 AM on the 15th day of \n\n \n \nPage 22 of 38 \n \na given month, while relevant high temperature was set for 6:00 PM of same day.  From these \nanchor points, outside air temperature was linearly distributed to all 3-minute intervals represented \nbetween these monthly centers, with ratable temperature increase from 6:00 AM to 6:00 PM, and \nratable decrease from 6:00 PM to 6:00 AM. This sampling is depicted in figures 27, 28, and 29. \nFigure 27 \n \nFigure 28 \n \n70\n74\n73\n66\n55\n91\n95\n95\n88\n78\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nOutside Air Temperature\nSource:  NOAA, Denton TX average temperatures\nDenton, TX average monthly low (left) & high (right) \ntemperatures - partial\n91.27\n69.14\n91.03\n68.90\n90.8\n68.66\n90.57\n68.41\n90.33\n68.17\n90.1\n67.93\n0\n20\n40\n60\n80\n100\nDay1\nPM\nDay2\nAM\nDay2\nPM\nDay3\nAM\nDay3\nPM\nDay4\nAM\nDay4\nPM\nDay5\nAM\nDay5\nPM\nDay6\nAM\nDay6\nPM\nDay7\nAM\nOutside Air Temperature\nDay & Time of week in late summer\nExample of simulating outside-air temperature, 9/1 through \n9/7/2020 (season of lowering temperatures)\n\n \n \nPage 23 of 38 \n \nFigure 29 \n \nAs is more apparent in this shorter time window, during a season of overall rising temperatures, the \nratable drop in interval temperature following the high temperature point of the day is nominally less \nthat were the ratable increases in interval temperature preceding the high temperature point of the \nday.  The opposite follows during a season of lowering temperatures. \nHumidity averages were applied evenly throughout the month identified to all 3-minute intervals \ncontained in said month, depicted in Figure 30.6 \n \n6 https://www.weather-us.com/en/texas-usa/denton-climate#humidity_relative \n88.890\n89.300\n89.720\n90.140\n90.570\n90.150\n89.730\n89.320\n88.900\n88.000\n88.500\n89.000\n89.500\n90.000\n90.500\n91.000\nT - 20\nT - 15\nT - 10\nT - 5\nT\nT + 5\nT + 10\nT + 15\nT + 20\nOutside Air Temperature\nT (daily high temperature) and # of 3-minute intervals reflecting a 2-hour spread\nHypothetical 3-minute intervals during season of rising \ntemperatures\n\n \n \nPage 24 of 38 \n \nFigure 30 \n \n \nWith outside air temperature (OAT) and humidity1 (H1) replaced by these values, a review of the \naverage relationship between OAT and temp1, temp2 resulted in extrapolation of values for those \nadditional columns; similarly, a review of H1 to humidity2 resulted in an extrapolation to populate \nhumidity2. \nFigure 31 represents the average results for manual calculations simulating the Macy‚Äôs dataset. \nFigure 31", "tokens": 821, "chars": 2809, "pages": {"start": 29, "end": 32}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0019", "next_id": "umontana-milestone2-validation_chunk_0021", "content_hash": "sha256:47ac443af5696a86dad3b88a6a6af362144158c638f8cb3dba03221f892e76c4"}
{"chunk_id": "umontana-milestone2-validation_chunk_0021", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 21, "text": "d temp1, temp2 resulted in extrapolation of values for those \nadditional columns; similarly, a review of H1 to humidity2 resulted in an extrapolation to populate \nhumidity2. \nFigure 31 represents the average results for manual calculations simulating the Macy‚Äôs dataset. \nFigure 31 \n \n\n \n \nPage 25 of 38 \n \nA histogram of the OAT (Figure 32) results into bins provided a reasonable approach for any given \n3-minute ‚Äúon‚Äù interval to identify the number of compressors ‚Äúactive‚Äù (identified with a value = 1).  A \nreview of the source data velocities, coupled with the number of compressors active provided a \nmeans to assign a relevant velocity measure, with variance allowed for based on differing humidity \nlevels by month. \nFigure 32 \n \nResulting compressor count and velocity calculation: \ncompressor 1 active as per Macy‚Äôs actual data:  600 * humidity1 \nIf OAT >= 78.5 = 2 compressors:  600 * humidity1 * 1.5 \nIf OAT >= 85.5 = 3 compressors:  600 * humidity1 * 2.0 \nIf OAT > 90 = 4 compressors:  600 * humidity1 * 2.5 \nThe remaining 6 source data variables (current and voltage readings) were taken from the original \nsource data as no significant anomalies were detected. \nWith these 12 variables for each 3-minute interval (166,430 of which 39,853 were compressor \nactive), manual calculations of all interim and final KPI‚Äôs were performed and captured, leading to a \n38-field simulation data set. \nA random sample of 8,000 active records was then subjected to a process of additional calculation \n(where OAT was arbitrarily increased by 15 degrees for calculation purposes only), with the resulting \nKPI‚Äôs being labeled as anomalies.  An additional field was created to label each record as a ‚Äú1‚Äù \n(8,000 anomaly records) or a ‚Äú0‚Äù (31,853 non-anomaly records). \nThe simulation model for calculating anomaly values in the KPI‚Äôs is relatively easy to adjust and \nother methods to calculate the anomalies may be useful for building a machine learning ‚Äútrain/test‚Äù \nmodel that could be later applied to the actual Macy‚Äôs data. \nConclusion on Simulated Data Set \nWhen MAD and Random Forest were applied to the labeled simulated data set, unfortunately results \nproduced were inconclusive. The manually created ‚Äòanomalies‚Äô were not identified in a consistent \nmanner by either algorithm and further exploration via modifications to the simulation set were not \npossible due to time constraints.", "tokens": 555, "chars": 2394, "pages": {"start": 32, "end": 33}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0020", "next_id": "umontana-milestone2-validation_chunk_0022", "content_hash": "sha256:d8809b3285fc97126b838e2ec78c14493e93db56585478d05b0c6cef51da524b"}
{"chunk_id": "umontana-milestone2-validation_chunk_0022", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 22, "text": "esults \nproduced were inconclusive. The manually created ‚Äòanomalies‚Äô were not identified in a consistent \nmanner by either algorithm and further exploration via modifications to the simulation set were not \npossible due to time constraints. \n\n \n \nPage 26 of 38 \n \nThus, the end conclusion on the manufactured anomalies and the simulated dataset was that the \ncreation of this dataset would need to be either further explored and tested or abandoned. The \nsimplest resolve would be to ensure real annotated data is collected as opposed to attempting to \ncreate a simulated set using the methodology described above. The limitation of the requirement for \nadditional, labeled data to further examine methodologies explored was not resolved with this \napproach. Therefore, the need for labeled datasets for future exploration of methods stands. \n \nOverall Limitations, Variables to Measure, and Conclusion \n \nLimitations \n \nThe standing limitation for the testing and exploration of all methodologies for anomaly detection was \nthe lack of labeled data. The need for an understanding of real-life decisions made by building \nengineers, device failures, and more would aid in assessing the actual success of certain algorithms, \nespecially those that function as supervised machine learning models. \n \nTime also proved to be a limitation, as the team had to first explore the data and possibilities for \nanalysis as well as understand the HVAC process for interpretation of results. If data with notes, or \neven a log made by the building engineer with dates and times included, was produced, it would be \npossible to execute further examination of outliers or provide meaning to the clusters that an \nalgorithm creates. Without this understanding, results bear success in a superficial manner that \nwould benefit greatly from an on-the-ground understanding of the device‚Äôs behavior and potential \nfactors impacting this behavior.  \n \nAdditional Variables to Actively Measure \n \nPowerTron expressed explicit interest in understanding whether actively taking additional \nmeasurements would aid in future explorations. The following bulleted list represents possible items \nto measure actively that would add value to the datasets currently being produced by PowerTron‚Äôs \nsensor technology: \n \n- \nAmount of time daily that each compressor is spent on \n- \nMAD for a selected increment (daily or monthly) \n- \nMaximum and Minimum daily values for each variable \n- \nDaily averages and medians for all variables \n- \nAny record of mechanical failure or sensor failure \n- \nPressure (needing additional sensor equipment) \n \nConclusion \n \nWhile the production of simulated data did not produce any additional results, preliminary results \nseen in the exploration of MAD, Random Forest, agglomerative hierarchical clustering, and \narchetypal analysis were promising and presented room for additional exploration.  \n \nPossible use cases for such algorithms range from the ability to notify individuals of a breach in \nthreshold in real time to the ability to examine the impact of usage decisions and changes to a \n\n \n \nPage 27 of 38 \n \ndevice within the framework of historical device behavior. Several methods left space for deeper \nexploration to potential develop a method for predictive mechanical failure, however limitations \nmentioned would need to be addressed prior to implementing such exploration. \n \nSuccessful application of a variety of analysis methods represents a true positive outcome of this \nproject and opens the door to many possibilities in the world of analyzing HVAC sensor data and \napplying it to improving current business offerings as well as the potential for offering an entirely new \nservice to existing customers if sensors were maintained post-treatment. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \nPage 28 of 38 \n \nAppendix A. Mathematical Formulation of Archetypes \n \nMathematical Formulation \n \n\n \n \nPage 29 of 38", "tokens": 750, "chars": 3970, "pages": {"start": 33, "end": 37}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0021", "next_id": "umontana-milestone2-validation_chunk_0023", "content_hash": "sha256:a84a183e2981ca9232bb5c716ad78eec5aad3efd782dcc36f5767ac4fb72698c"}
{"chunk_id": "umontana-milestone2-validation_chunk_0023", "doc_id": "umontana-milestone2-validation", "slug": "umontana-milestone2-validation", "sequence": 23, "text": "AC sensor data and \napplying it to improving current business offerings as well as the potential for offering an entirely new \nservice to existing customers if sensors were maintained post-treatment. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \nPage 28 of 38 \n \nAppendix A. Mathematical Formulation of Archetypes \n \nMathematical Formulation \n \n\n \n \nPage 29 of 38 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\n \n \nPage 30 of 38 \n \nAppendix B. Archetypal Analysis References \n \nBauckhage, C., 2014: A note on archetypal analysis and the approximation of convex hulls, \narxiv:1410.0642 \nCutler, A. and L. Breiman, 1994: Archetypal analysis. Technometrics, 36(4), 338‚Äì347. \n \nCutler, A. and E. Stone, 1997: Moving archetypes. Physica D: Nonlinear Phenomena, 107(1), 1 \n‚Äì16. \n \nEpifanio, I., G. Vinue, and S. Alemany, 2013: Archetypal analysis: Contributions for estimating \nboundary cases in multivariate accommodation problem. Computers & Industrial Engineering, \n64(3), \n757 ‚Äì 765. \n \nHannachi, A. and N. Trendafilov, 2017: Archetypal Analysis: Mining Weather and Climate \nExtremes. \nJournal of Climate, 30(17), 6927‚Äì6944. \n \nLundberg, R., 2019: Archetypal terrorist events in the united states. Studies in Conflict & \nTerrorism, 42(9), 819‚Äì835. \n \nM√∏rup, M. and L. K. Hansen, 2012: Archetypal analysis for machine learning and data mining. \nNeurocomputing, 80, 54 ‚Äì 63, special Issue on Machine Learning for Signal Processing 2010. \n \nSeth, S. and M. J. A. Eugster, 2016: Probabilistic archetypal analysis. Machine Learning, \n102(1), 85‚Äì113. \n \nSteinschneider, S. and U. Lall, 2015: Daily Precipitation and Tropical Moisture Exports across \nthe Eastern United States: An Application of Archetypal Analysis to Identify Spatiotemporal \nStructure. Journal of Climate, 28(21), 8585‚Äì8602. \n \nStone, E. and A. Cutler, 1996: Archetypal analysis of spatio-temporal dynamics. Physica D: \nNonlinear Phenomena, 90(3), 209 ‚Äì 224. \n \nTh√∏gersen, J. C., M. M√∏rup, S. Damki√¶r, S. Molin, and L. Jelsbak, 2013: Archetypal analysis of \ndiverse pseudomonas aeruginosatranscriptomes reveals adaptation in cystic fibrosis airways. \nBMC Bioinformatics, 14(1), 279. \n \nVinue, G., I. Epifanio, and S. Alemany, 2015: Archetypoids: A new approach to define \nrepresentative archetypal data. Computational Statistics & Data Analysis, 87, 102 ‚Äì 115.", "tokens": 735, "chars": 2361, "pages": {"start": 35, "end": 38}, "section": "", "prev_id": "umontana-milestone2-validation_chunk_0022", "next_id": "", "content_hash": "sha256:0dfde1bf9d57af41aa71797fb882e4a5222b452d857ad80b670b8b2c232f7e13"}
